import os
import sys
import json
from pathlib import Path
from datetime import datetime

# è¨­å®šå°ˆæ¡ˆè·¯å¾‘
sys.path.append(os.getcwd())

from models.database import db
from core.parser import parse_document
from core.chunker import chunk_document_with_pages
from core.embedder import embed_texts
from core.vectordb import add_chunks

def reindex_failed_documents():
    """é‡æ–°ç´¢å¼•å¤±æ•—çš„æ–‡ä»¶"""
    
    # 1. æŸ¥è©¢å¤±æ•—çš„æ–‡ä»¶ (é™åˆ¶ PDF ä¸”ä¸€æ¬¡è™•ç† 5 å€‹)
    with db.get_connection() as conn:
        failed_docs = conn.execute(
            "SELECT * FROM documents WHERE status = 'failed' AND type = 'pdf' LIMIT 5"
        ).fetchall()
        
    if not failed_docs:
        print("âœ… æ²’æœ‰å¤±æ•—çš„æ–‡ä»¶éœ€è¦è™•ç†ã€‚")
        return

    print(f"ğŸ”„ ç™¼ç¾ {len(failed_docs)} å€‹å¤±æ•—çš„æ–‡ä»¶ï¼Œæº–å‚™é‡æ–°ç´¢å¼•...\n")
    
    success_count = 0
    
    for row in failed_docs:
        doc = dict(row)
        filepath = doc['filepath']
        doc_id = doc['id']
        filename = doc['filename']
        
        print(f"ğŸ‘‰ æ­£åœ¨è™•ç†: {filename} ...")
        
        try:
            # 2. è§£ææ–‡ä»¶
            print(f"   - è§£æä¸­...", end='\r')
            parsed = parse_document(filepath)
            
            if not parsed or not parsed.get('text'):
                print(f"   âŒ è§£æå¤±æ•—: ç„¡æ³•æå–æ–‡å­—")
                continue
                
            # 3. åˆ‡åˆ† (ä¿ç•™é ç¢¼)
            print(f"   - åˆ‡åˆ†ä¸­...   ", end='\r')
            chunks = chunk_document_with_pages(
                parsed.get('pages', []),
                chunk_size=500, # ä½¿ç”¨é è¨­æˆ–å¾ config è®€
                use_smart_chunking=True
            )
            
            if not chunks:
                print(f"   âŒ åˆ‡åˆ†å¤±æ•—: ç„¡æœ‰æ•ˆ chunk")
                continue
            
            # 4. è½‰å‘é‡
            print(f"   - å‘é‡åŒ–ä¸­ ({len(chunks)} chunks)...", end='\r')
            texts = [c['text'] for c in chunks]
            embeddings = embed_texts(texts)
            
            # 5. å¯«å…¥å‘é‡è³‡æ–™åº«
            print(f"   - å¯«å…¥è³‡æ–™åº«...", end='\r')
            
            # è£œå…… metadata
            for chunk in chunks:
                chunk['metadata'].update({
                    'filename': filename,
                    'folder': doc.get('folder'),
                    'type': doc.get('type')
                })
                
            add_chunks(doc_id, chunks, embeddings)
            
            # 6. æ›´æ–°ç‹€æ…‹
            db.update_document_status(doc_id, 'indexed', chunks_count=len(chunks))
            
            print(f"   âœ… æˆåŠŸç´¢å¼•ï¼ ({len(chunks)} chunks)      ")
            success_count += 1
            
        except Exception as e:
            print(f"   âŒ è™•ç†ç™¼ç”ŸéŒ¯èª¤: {str(e)}           ")
            import traceback
            # traceback.print_exc()
            
    print("-" * 50)
    print(f"ğŸ å®Œæˆã€‚æˆåŠŸ: {success_count}, å¤±æ•—: {len(failed_docs) - success_count}")

if __name__ == "__main__":
    reindex_failed_documents()
