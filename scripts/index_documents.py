#!/usr/bin/env python3
"""
Knowledge Hub - Memory Optimized Document Indexing CLI

ç¨ç«‹çš„CLIå·¥å…·ï¼Œç”¨æ–¼ç´¢å¼•æ–‡æª”åˆ°å‘é‡è³‡æ–™åº«ï¼Œé¿å…è¨˜æ†¶é«”è€—ç›¡å•é¡Œã€‚
ä¸ä¾è³´Flaskæ‡‰ç”¨ï¼Œå°ˆé–€è™•ç†å¤§é‡æ–‡æª”çš„æ‰¹æ¬¡ç´¢å¼•ä½œæ¥­ã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/index_documents.py --resume
    python scripts/index_documents.py --full-reindex --batch-size 3
    python scripts/index_documents.py --single-file "documents/example.pdf"
"""

import os
import sys
import json
import time
import argparse
import psutil
import traceback
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple

# è¨­å®šå°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from config import Config
from models.database import db
from core.scanner import scan_directory, sync_documents
from core.parser import parse_document
from core.chunker import chunk_document_with_pages
from core.embedder import embed_texts
from core.vectordb import add_chunks, delete_document_chunks


class MemoryMonitor:
    """è¨˜æ†¶é«”ä½¿ç”¨é‡ç›£æ§"""

    def __init__(self, max_memory_mb: Optional[int] = None):
        self.max_memory_mb = max_memory_mb or (psutil.virtual_memory().available // (1024 * 1024) * 0.8)  # 80%å¯ç”¨è¨˜æ†¶é«”
        self.process = psutil.Process()
        self.start_memory = self.get_memory_usage()

    def get_memory_usage(self) -> int:
        """å–å¾—ç›®å‰è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)"""
        return self.process.memory_info().rss // (1024 * 1024)

    def check_memory_limit(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦è¶…éè¨˜æ†¶é«”é™åˆ¶"""
        current = self.get_memory_usage()
        return current > self.max_memory_mb

    def get_memory_stats(self) -> Dict:
        """å–å¾—è¨˜æ†¶é«”çµ±è¨ˆ"""
        current = self.get_memory_usage()
        return {
            'current_mb': current,
            'max_allowed_mb': self.max_memory_mb,
            'usage_percent': (current / self.max_memory_mb) * 100 if self.max_memory_mb > 0 else 0,
            'available_mb': psutil.virtual_memory().available // (1024 * 1024)
        }


class DocumentIndexer:
    """æ–‡æª”ç´¢å¼•å™¨ - è¨˜æ†¶é«”å„ªåŒ–ç‰ˆæœ¬"""

    def __init__(self, memory_monitor: MemoryMonitor, batch_size: int = 5, verbose: bool = False):
        self.memory_monitor = memory_monitor
        self.batch_size = batch_size
        self.verbose = verbose
        self.stats = {
            'processed': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'memory_peaks': []
        }

    def log(self, message: str, level: str = 'INFO'):
        """è¨˜éŒ„è¨Šæ¯"""
        if self.verbose or level in ['ERROR', 'WARNING']:
            timestamp = datetime.now().strftime('%H:%M:%S')
            print(f"[{timestamp}] {level}: {message}")

    def check_file_size_limit(self, filepath: str, max_size_mb: int = 50) -> bool:
        """æª¢æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦è¶…éé™åˆ¶"""
        size_mb = Path(filepath).stat().st_size / (1024 * 1024)
        if size_mb > max_size_mb:
            self.log(f"æ–‡ä»¶éå¤§ ({size_mb:.1f}MB > {max_size_mb}MB): {filepath}", 'WARNING')
            return False
        return True

    def cleanup_memory(self):
        """æ‰‹å‹•æ¸…ç†è¨˜æ†¶é«”"""
        import gc
        gc.collect()

        # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨å³°å€¼
        mem_stats = self.memory_monitor.get_memory_stats()
        self.stats['memory_peaks'].append(mem_stats['current_mb'])

        if self.verbose:
            self.log(f"è¨˜æ†¶é«”æ¸…ç†å®Œæˆï¼Œç›®å‰ä½¿ç”¨: {mem_stats['current_mb']}MB")

    def process_single_document(self, doc: Dict) -> Tuple[bool, str]:
        """
        è™•ç†å–®ä¸€æ–‡æª”

        Args:
            doc: æ–‡æª”è³‡è¨Šå­—å…¸

        Returns:
            (æˆåŠŸæ¨™è¨˜, éŒ¯èª¤è¨Šæ¯)
        """
        doc_id = doc['id']
        filepath = doc['filepath']
        filename = doc['filename']

        try:
            # æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨é‡
            if self.memory_monitor.check_memory_limit():
                return False, "è¨˜æ†¶é«”ä½¿ç”¨é‡éé«˜"

            # æª¢æŸ¥æ–‡ä»¶å¤§å°
            if not self.check_file_size_limit(filepath):
                return False, "æ–‡ä»¶éå¤§"

            self.log(f"è™•ç†æ–‡æª”: {filename} (ID: {doc_id})")

            # æ›´æ–°ç‹€æ…‹ç‚ºè™•ç†ä¸­
            db.update_document_status(doc_id, 'processing')

            # 1. è§£ææ–‡æª”
            self.log(f"  - è§£ææ–‡æª”ä¸­: {filepath}", 'DEBUG')
            try:
                parsed = parse_document(filepath)
                if parsed and parsed.get('text'):
                    self.log(f"    è§£ææˆåŠŸï¼Œæ–‡å­—é•·åº¦: {len(parsed['text'])} å­—å…ƒï¼Œé æ•¸: {len(parsed.get('pages', []))}", 'DEBUG')
                else:
                    self.log(f"    è§£æå¤±æ•—æˆ–ç„¡æ–‡å­—å…§å®¹", 'WARNING')
            except Exception as parse_e:
                self.log(f"    è§£æç•°å¸¸: {str(parse_e)}", 'ERROR')
                raise

            if not parsed or not parsed.get('text'):
                error_msg = "è§£æå¤±æ•—ï¼šç„¡æ³•æå–æ–‡å­—"
                self.log(f"   âŒ {error_msg}")
                db.update_document_status(doc_id, 'failed')
                return False, error_msg

            # 3. åˆ‡åˆ†æ–‡æª”
            self.log(f"   - åˆ‡åˆ†ä¸­...", 'DEBUG')
            chunks = chunk_document_with_pages(
                parsed.get('pages', []),
                chunk_size=Config.CHUNK_SIZE,
                use_smart_chunking=True
            )

            if not chunks:
                error_msg = "åˆ‡åˆ†å¤±æ•—ï¼šç„¡æœ‰æ•ˆchunk"
                self.log(f"   âŒ {error_msg}")
                db.update_document_status(doc_id, 'failed')
                return False, error_msg

            # 2. åˆ‡åˆ†æ–‡æª”
            self.log(f"  - åˆ‡åˆ†ä¸­...", 'DEBUG')
            chunks = chunk_document_with_pages(
                parsed.get('pages', []),
                chunk_size=Config.CHUNK_SIZE,
                use_smart_chunking=True
            )

            if not chunks:
                error_msg = "åˆ‡åˆ†å¤±æ•—ï¼šç„¡æœ‰æ•ˆchunk"
                self.log(f"  âŒ {error_msg}")
                # Update with error info - implementation depends on database schema
                # For now, just mark as failed
                db.update_document_status(doc_id, 'failed')
                return False, error_msg

            # 3. åˆ†æ‰¹ç”ŸæˆåµŒå…¥å‘é‡ (è¨˜æ†¶é«”å„ªåŒ–)
            self.log(f"  - å‘é‡åŒ– {len(chunks)} chunks...", 'DEBUG')
            texts = [c['text'] for c in chunks]
            embeddings = []

            # åˆ†æ‰¹è™•ç†åµŒå…¥
            batch_size = Config.EMBEDDING_BATCH_SIZE
            total_embeddings = 0
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                self.log(f"    è™•ç†åµŒå…¥æ‰¹æ¬¡ {i//batch_size + 1}/{(len(texts) + batch_size - 1)//batch_size} ({len(batch_texts)} å€‹æ–‡å­—)", 'DEBUG')
                try:
                    batch_embeddings = embed_texts(batch_texts)
                    embeddings.extend(batch_embeddings)
                    total_embeddings += len(batch_embeddings)
                    self.log(f"    æ‰¹æ¬¡åµŒå…¥æˆåŠŸï¼Œç´¯è¨ˆ: {total_embeddings}/{len(texts)}", 'DEBUG')
                except Exception as embed_e:
                    self.log(f"    åµŒå…¥ç•°å¸¸: {str(embed_e)}", 'ERROR')
                    raise

            if len(embeddings) != len(chunks):
                raise ValueError(f"åµŒå…¥æ•¸é‡ä¸åŒ¹é…: æœŸæœ› {len(chunks)}, å¯¦éš› {len(embeddings)}")

                # æª¢æŸ¥è¨˜æ†¶é«”
                if self.memory_monitor.check_memory_limit():
                    error_msg = "åµŒå…¥éç¨‹ä¸­è¨˜æ†¶é«”ä¸è¶³"
                    self.log(f"   âŒ {error_msg}")
                    db.update_document_status(doc_id, 'failed')
                    return False, error_msg

            # 4. å„²å­˜åˆ°å‘é‡è³‡æ–™åº«
            self.log(f"  - å„²å­˜åˆ°å‘é‡è³‡æ–™åº«...", 'DEBUG')

            # è£œå……metadata
            for chunk in chunks:
                chunk['metadata'].update({
                    'filename': filename,
                    'folder': doc.get('folder'),
                    'type': doc.get('type')
                })

            add_chunks(doc_id, chunks, embeddings)

            # 5. æ›´æ–°ç‹€æ…‹
            db.update_document_status(doc_id, 'indexed', chunks_count=len(chunks))

            self.log(f"  âœ… æˆåŠŸç´¢å¼• {len(chunks)} chunks")
            return True, ""

        except Exception as e:
            error_details = traceback.format_exc()
            error_msg = f"è™•ç†ç•°å¸¸: {str(e)}\nè©³ç´°éŒ¯èª¤:\n{error_details}"
            self.log(f"  âŒ {error_msg}", 'ERROR')
            db.update_document_status(doc_id, 'failed')
            return False, error_msg

        finally:
            # æ¸…ç†è¨˜æ†¶é«”
            self.cleanup_memory()

    def process_batch(self, documents: List[Dict], job_id: str) -> Dict:
        """
        æ‰¹æ¬¡è™•ç†æ–‡æª”

        Args:
            documents: è¦è™•ç†çš„æ–‡æª”åˆ—è¡¨
            job_id: ä½œæ¥­IDï¼Œç”¨æ–¼é€²åº¦è¿½è¹¤

        Returns:
            è™•ç†çµ±è¨ˆ
        """
        batch_stats = {
            'total': len(documents),
            'successful': 0,
            'failed': 0,
            'skipped': 0
        }

        self.log(f"é–‹å§‹è™•ç†æ‰¹æ¬¡ï¼Œå…± {len(documents)} å€‹æ–‡æª”")

        for i, doc in enumerate(documents):
            doc_id = doc['id']
            filename = doc['filename']

            self.log(f"[{i+1}/{len(documents)}] è™•ç†: {filename}")

            # è™•ç†å–®ä¸€æ–‡æª”
            success, error_msg = self.process_single_document(doc)

            if success:
                batch_stats['successful'] += 1
                self.stats['successful'] += 1
            else:
                batch_stats['failed'] += 1
                self.stats['failed'] += 1
                self.log(f"æ–‡æª”è™•ç†å¤±æ•—: {filename} - {error_msg}", 'ERROR')

            self.stats['processed'] += 1

            # æ›´æ–°ä½œæ¥­é€²åº¦
            if job_id:
                db.update_job(job_id, processed=self.stats['processed'])

            # æ‰¹æ¬¡é–“çš„å°æš«åœï¼Œè®“ç³»çµ±æ¢å¾©
            time.sleep(0.1)

        return batch_stats

    def get_pending_documents(self, limit: Optional[int] = None) -> List[Dict]:
        """å–å¾—å¾…è™•ç†çš„æ–‡æª”"""
        result = db.get_documents(status='pending', limit=limit or 1000)
        return result['documents']

    def get_failed_documents(self, limit: Optional[int] = None) -> List[Dict]:
        """å–å¾—å¤±æ•—çš„æ–‡æª”"""
        result = db.get_documents(status='failed', limit=limit or 1000)
        return result['documents']


def create_index_job(total_files: int) -> str:
    """å»ºç«‹ç´¢å¼•ä½œæ¥­"""
    job_id = f"job_{int(time.time())}"
    db.create_job(job_id, total_files)
    return job_id


def main():
    """ä¸»ç¨‹å¼"""
    job_id = None  # åˆå§‹åŒ–ä»¥é¿å… linter éŒ¯èª¤

    parser = argparse.ArgumentParser(
        description="Knowledge Hub - è¨˜æ†¶é«”å„ªåŒ–æ–‡æª”ç´¢å¼•CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  python scripts/index_documents.py --resume                    # ç¹¼çºŒä¸Šæ¬¡ä½œæ¥­
  python scripts/index_documents.py --full-reindex --batch-size 3  # é‡æ–°ç´¢å¼•æ‰€æœ‰ï¼Œæ‰¹æ¬¡å¤§å°3
  python scripts/index_documents.py --single-file "documents/example.pdf"  # è™•ç†å–®ä¸€æ–‡ä»¶
  python scripts/index_documents.py --dry-run --verbose         # æ¨¡æ“¬é‹è¡Œï¼Œè©³ç´°è¼¸å‡º
        """
    )

    parser.add_argument('--resume', action='store_true',
                       help='ç¹¼çºŒä¸Šæ¬¡æœªå®Œæˆçš„ä½œæ¥­ (é è¨­)')
    parser.add_argument('--full-reindex', action='store_true',
                       help='é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡ä»¶')
    parser.add_argument('--single-file', type=str,
                       help='åªè™•ç†æŒ‡å®šçš„å–®ä¸€æ–‡ä»¶')
    parser.add_argument('--batch-size', type=int, default=5,
                       help='æ¯æ‰¹è™•ç†çš„æ–‡ä»¶æ•¸é‡ (é è¨­: 5)')
    parser.add_argument('--max-memory', type=int,
                       help='è¨˜æ†¶é«”é™åˆ¶ (MB)ï¼Œé è¨­ç‚ºå¯ç”¨è¨˜æ†¶é«”çš„80%')
    parser.add_argument('--dry-run', action='store_true',
                       help='æ¨¡æ“¬é‹è¡Œï¼Œä¸å¯¦éš›è™•ç†æ–‡ä»¶')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è©³ç´°è¼¸å‡º')

    args = parser.parse_args()

    # è¨­å®šé è¨­è¡Œç‚º
    if not any([args.resume, args.full_reindex, args.single_file]):
        args.resume = True

    # åˆå§‹åŒ–
    print("ğŸ”„ Knowledge Hub - è¨˜æ†¶é«”å„ªåŒ–æ–‡æª”ç´¢å¼•å™¨")
    print(f"ğŸ“ æƒæç›®éŒ„: {Config.SCAN_DIR}")
    print(f"ğŸ’¾ è³‡æ–™ç›®éŒ„: {Config.DATA_DIR}")
    print()

    # åˆå§‹åŒ–è¨˜æ†¶é«”ç›£æ§
    memory_monitor = MemoryMonitor(args.max_memory)

    if args.verbose:
        mem_stats = memory_monitor.get_memory_stats()
        print(f"ğŸ§  è¨˜æ†¶é«”é™åˆ¶: {mem_stats['max_allowed_mb']}MB")
        print(f"ğŸ“Š å¯ç”¨è¨˜æ†¶é«”: {mem_stats['available_mb']}MB")
        print()

    # åˆå§‹åŒ–ç´¢å¼•å™¨
    indexer = DocumentIndexer(memory_monitor, args.batch_size, args.verbose)

    try:
        # æƒæä¸¦åŒæ­¥æ–‡ä»¶
        if not args.dry_run:
            print("ğŸ” æƒæä¸¦åŒæ­¥æ–‡ä»¶åˆ°è³‡æ–™åº«...")
            sync_result = sync_documents(scan_path=str(Config.SCAN_DIR), recursive=True)
            print(f"ğŸ“„ ç™¼ç¾ {sync_result['total_files']} å€‹æ–‡ä»¶ï¼Œæ–°å¢ {sync_result['new_files']} å€‹")
            print()

        # æ±ºå®šè¦è™•ç†çš„æ–‡ä»¶
        documents_to_process = []

        if args.single_file:
            # è™•ç†å–®ä¸€æ–‡ä»¶
            filepath = Path(args.single_file)
            if not filepath.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.single_file}")
                return 1

            # æƒæä¸¦åŒæ­¥è©²æ–‡ä»¶
            if not args.dry_run:
                sync_documents(scan_path=str(filepath.parent), recursive=False)

            # å¾è³‡æ–™åº«å–å¾—è©²æ–‡ä»¶
            doc_id = None  # éœ€è¦ç”Ÿæˆdoc_id
            from core.scanner import generate_doc_id
            doc_id = generate_doc_id(str(filepath))

            doc = db.get_document(doc_id)
            if doc:
                documents_to_process = [doc]
            else:
                print(f"âŒ ç„¡æ³•åœ¨è³‡æ–™åº«ä¸­æ‰¾åˆ°æ–‡ä»¶: {args.single_file}")
                return 1

        elif args.full_reindex:
            # é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡ä»¶
            print("ğŸ”„ é‡æ–°ç´¢å¼•æ‰€æœ‰æ–‡ä»¶...")
            if not args.dry_run:
                # å°‡æ‰€æœ‰indexedæ–‡ä»¶é‡è¨­ç‚ºpending
                with db.get_connection() as conn:
                    conn.execute("UPDATE documents SET status = 'pending' WHERE status = 'indexed'")

            documents_to_process = indexer.get_pending_documents()

        else:  # args.resume (é è¨­)
            # ç¹¼çºŒæœªå®Œæˆçš„ä½œæ¥­
            print("â–¶ï¸ ç¹¼çºŒæœªå®Œæˆçš„ä½œæ¥­...")

            # å–å¾—pendingå’Œfailedæ–‡ä»¶
            pending_docs = indexer.get_pending_documents()
            failed_docs = indexer.get_failed_documents()

            documents_to_process = pending_docs + failed_docs

            if not documents_to_process:
                print("âœ… æ²’æœ‰å¾…è™•ç†çš„æ–‡ä»¶")
                return 0

        if not documents_to_process:
            print("âŒ æ²’æœ‰æ‰¾åˆ°è¦è™•ç†çš„æ–‡ä»¶")
            return 1

        print(f"ğŸ“‹ å¾…è™•ç†æ–‡ä»¶æ•¸é‡: {len(documents_to_process)}")
        print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        if args.dry_run:
            print("ğŸ§ª é€™æ˜¯æ¨¡æ“¬é‹è¡Œï¼Œä¸æœƒå¯¦éš›è™•ç†æ–‡ä»¶")
        print()

        # å»ºç«‹ä½œæ¥­
        job_id = None
        if not args.dry_run and not args.single_file:
            job_id = create_index_job(len(documents_to_process))
            print(f"ğŸ“ ä½œæ¥­ID: {job_id}")
            print()

        # æ‰¹æ¬¡è™•ç†
        start_time = time.time()
        total_batches = (len(documents_to_process) + args.batch_size - 1) // args.batch_size

        for batch_idx in range(total_batches):
            batch_start = batch_idx * args.batch_size
            batch_end = min(batch_start + args.batch_size, len(documents_to_process))
            batch_docs = documents_to_process[batch_start:batch_end]

            print(f"ğŸ”„ è™•ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} ({len(batch_docs)} å€‹æ–‡ä»¶)")

            if args.dry_run:
                # æ¨¡æ“¬è™•ç†
                for doc in batch_docs:
                    indexer.log(f"æ¨¡æ“¬è™•ç†: {doc['filename']}")
                    time.sleep(0.1)  # æ¨¡æ“¬è™•ç†æ™‚é–“
                batch_stats = {'successful': len(batch_docs), 'failed': 0, 'skipped': 0}
            else:
                # å¯¦éš›è™•ç†
                batch_stats = indexer.process_batch(batch_docs, job_id or "")

            print(f"   âœ… æˆåŠŸ: {batch_stats['successful']}, âŒ å¤±æ•—: {batch_stats['failed']}")
            print()

            # æ‰¹æ¬¡é–“çš„è¨˜æ†¶é«”æª¢æŸ¥
            mem_stats = memory_monitor.get_memory_stats()
            if mem_stats['usage_percent'] > 90:
                indexer.log("è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜ï¼Œå¼·åˆ¶æ¸…ç†...", 'WARNING')
                indexer.cleanup_memory()

        # å®Œæˆä½œæ¥­
        if not args.dry_run and job_id:
            db.update_job(job_id, status='completed')

        # ç¸½çµ
        elapsed_time = time.time() - start_time

        print("ğŸ è™•ç†å®Œæˆ!")
        print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed_time:.1f} ç§’")
        print(f"ğŸ“Š ç¸½è™•ç†: {indexer.stats['processed']} å€‹æ–‡ä»¶")
        print(f"âœ… æˆåŠŸ: {indexer.stats['successful']}")
        print(f"âŒ å¤±æ•—: {indexer.stats['failed']}")

        if indexer.stats['memory_peaks']:
            max_memory = max(indexer.stats['memory_peaks'])
            print(f"ğŸ§  è¨˜æ†¶é«”å³°å€¼: {max_memory}MB")

        return 0 if indexer.stats['failed'] == 0 else 1

    except KeyboardInterrupt:
        print("\nâš ï¸ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨æ¸…ç†...")
        if 'job_id' in locals() and job_id and not args.dry_run:
            db.update_job(job_id, status='failed')
        return 130

    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        if 'job_id' in locals() and job_id and not args.dry_run:
            db.update_job(job_id, status='failed')
        return 1


if __name__ == '__main__':
    exit(main())